# 第 1 章 统计学习方法概论

## 1.1 统计学习的定义
<p>
统计学习也称为统计机器学习。
</p>

统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。

### 1. 统计学习的特点

<ul>
    <li>统计学习以计算机及网络为平台，是建立在计算机及网络之上的；
    <li>统计学习以数据为研究对象，是数据驱动的学科；
    <li>统计学习的目的是对数据进行预测与分析；
    <li>统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析；
    <li>统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。
</ul>

### 2. 统计学习的对象
<p>
<b>数据</b>
</p>

<p>
统计学习关于数据的基本假设是同类数据具有一定的<font color="red">统计规律性</font>，这是统计学习的前提。例如，可以用随机变量描述数据中的特征，用概率分布描述数据的统计规律。
</p>

<p>
在统计学习过程中，以变量或变量组表示数据。数据分为由连续变量和离散变量表示的类型。
</p>

### 3. 统计学习的目的
<p>统计学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析。对数据的预测可以使计算机更加智能化，或者说使计算机的某些性能得到提高；对数据的分析可以让人们获取新的知识，给人们带来新的发现。</p>

### 4. 统计学习的方法
<p>统计学习的方法是基于数据构建统计模型从而对数据进行预测与分析。统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）和强化学习（reinforcement learning）等组成。</p>

<p>统计学习方法的定义：从给定的、有限的、用于学习的<b>训练数据（training data）</b>集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为<b>假设空间（hypothesis space）</b>；应用某个<b>评价准则（evaluation criterion）</b>，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据（test data）在给定的评价准则下有最优的预测；最优模型的选取由算法实现。</p>

<p>统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，称其为统计学习方法的三要素，简称为模型（model）、策略（strategy）和算法（algorithm）</p>

实现统计学习方法的步骤如下：
<ul>
    <li>得到一个有限的训练数据集合；
    <li>确定包含所有可能的模型的假设空间，即学习模型的集合；
    <li>确定模型选择的准则，即学习的策略；
    <li>实现求解最优模型的算法，即学习的算法；
    <li>通过学习方法选择最优模型；
    <li>利用学习的最优模型对新数据进行预测或分析。
</ul>

<p>监督学习方法，主要包括用于<b>分类</b>、<b>标注</b>与<b>回归</b>问题的方法。这些方法在自然语言处理、信息检索、文本数据挖掘等领域中有着极其广泛的应用。</p>

### 5. 统计学习的研究
<p>统计学习研究一般包括统计学习方法、统计学习理论及统计学习应用三个方面。</p>

### 6. 统计学习的重要性
<p>近20年来，统计学习无论是在理论还是在应用方面都得到了巨大的发展，有许多重大突破，统计学习已被成功地应用到人工智能、模型识别、数据挖掘、自然语言处理、语音识别、图像识别、信息检索和生物信息等许多计算机应用领域中，并且成为这些领域的核心技术。</p>

<p>统计学习是计算机科学发展的一个重要组成部分。可以认为计算机科学由三维组成：<b>系统、计算、信息</b>。统计学习主要属于信息这一维，并在其中起着核心作用。</p>


## 1.2 监督学习

<p>统计学习包括<b>监督学习</b>、<b>非监督学习</b>、<b>半监督学习</b>及<b>强化学习</b>。</p>

<p>监督学习（supervised learning）的任务是学习一个模型，是模型能够对任意给定的输入，对其相应的输出做出一个好的预测（注意，这里的输入、输出是指<font color="red">某个系统</font>的输入与输出，与学习的输入与输出不同）。计算机的基本操作就是给定一个输入产生一个输出，所以监督学习是极其重要的统计学习分之，也是统计学习中内容最丰富、应用最广泛的部分。</p>

### 1.2. 基本概念

#### 1. 输入空间、特征空间与输出空间
<p>在监督学习中，将输入与输出所有可能取值的集合分别称为输入空间（input space）与输出空间（output space）。输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。输入空间与输出空间可以是同一个空间，也可以是不同的空间；但通常输出空间远远小于输入空间。</p>

<p>每个具体的输入是一个<b>实例（instance）</b>，通常由<b>特征向量（feature vector）</b>表示。这时，所有特征向量存在的空间称为<b>特征空间（feature space）</b>，特征空间的每一维对应于一个特征。有时假设输入空间与特征空间为相同的空间，对它们不予区分；有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。<font color="red">模型实际上都是定义在特征空间上的。</font></p>

<p>在监督学习过程中，将输入与输出看作是定义在输入（特征）空间与输出空间上的随机变量的取值。输入、输出变量用大写字母表示，习惯上输入变量写作 <b><i>X</i></b>，输出变量写作<b><i>Y</i></b>。输入、输出变量所取的值用小写字母表示，输入变量的取值写作<b><i>x</i></b>，输出变量的取值写作<b><i>y</i></b>。变量可以是标量或向量，都用相同类型字母表示。</p>

<p>输入变量<b><i>X</i></b>和输出变量<b><i>Y</i></b>有不同的类型，可以是连续的，也可以是离散的。人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：</p>

<ul>
    <li>X, Y 均为连续性，则为<b>回归问题</b>；
    <li>Y 为有限个离散变量，则为<b>分类问题</b>；
    <li>X, Y 均为变量序列，则为标注问题。
</ul>


#### 2. 联合概率分布
<p>监督学习假设输入与输出的随机变量<b><i>X</i></b>和<b><i>Y</i></b>遵循联合概率分布<b><i>P(X, Y)</i></b>。<b><i>P(X, Y)</i></b>表示分布函数，或分布密度函数。注意，在学习过程中，假定这一联合概率分布存在，<u>但对学习系统来说，联合概率分布的具体定义是未知的</u>。训练数据与测试数据被看作是依联合概率分布<b><i>P(X, Y)</i></b>产生的。统计学习假设数据存在一定的统计规律，<b><i>X</i></b>和<b><i>Y</i></b>具有联合概率分布的假设就是<u>监督学习</u>关于数据的基本假设。</p>


#### 3. 假设空间
<p>监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。换句话说，学习的目的就在于找到最好的这样的模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。假设空间的确定意味着学习范围的确定。</p>

<p>监督学习的模型可以是概率模型或非概率模型，由条件概率分布<b><i>P(Y|X)</i></b>或决策函数（decision function）<b><i>Y = f(X)</i></b>表示，随具体学习方法而定。对具体的输入进行相应的预测时，写作<b><i>P(y|x)</i></b>或<b><i>y=f(x)</i></b>。</p>

### 1.2.2 问题的形式化
监督学习分为学习和预测两个过程，由学习系统与预测系统完成。


## 1.3 统计学习三要素
<p>统计学习方法都是由模型、策略和算法构成的，即统计学习方法由三要素构成，可以简单地表示为：</p>

<p>
<b>方法 = 模型 + 策略 + 算法</b>
</p>

<p>下面论述监督学习中的统计学习三要素。非监督学习、强化学习也同样拥有这三要素。可以说构建一种统计学习方法就是确定具体的统计学习三要素。</p>


### 1.3.1 模型
<p>统计学习首要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个。</p>



### 1.3.2 策略
<p>有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。</p>

<p>首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测好坏。</p>



### 1.3.3 算法
<p>算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。</p>

<p>这时，统计学习问题归结为最优化问题，统计学习的算法称为求解最优化问题的算法。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用<font color="red">数值计算</font>的方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。统计学习可以利用已有的最优化算法，有时也需要开发独自的最优化算法。</p>

<p>统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了。这也就是将其称为统计学习三要素的原因。</p>


## 1.4 模型评估与模型选择


## 1.5 正则化与交叉验证

### 1.5.1 正则化
<p>模型选择的典型方法是正则化(regularization)。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。</p>


### 1.5.2 交叉验证
<p>另一种常用的模型选择方法是交叉验证（cross validation）。</p>
<p>如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。训练集用于模型的选择，而测试集用于最终对学习方法的评估。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。</p>
<p>但是，在许多实际应用中，数据是不充足的。为了选择好的模型，可以采用交叉验证的方法。交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p>

#### 1.简单交叉验证
<p>简单交叉验证方法是：首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集（例如，70%的数据为训练集，30%的数据为测试集）；然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。</p>

#### 2.S 折交叉验证
<p>应用最多的是S 折交叉验证（S - fold cross validation），方法如下：首先随机地将已给数据切分为 S 个互不相交的大小相同的自己；然后利用 S - 1 个子集的数据训练模型，利用余下的自己测试模型；将这一过程对可能的 S 种选择重复进行；最后选出 S 次评测中平均测试误差最小的模型。</p>

#### 3.留一交叉验证
<p>S 折交叉验证的特殊情形是 S = N，称为留一（N 折）交叉验证（leave-one-out cross validation），往往在<b>数据缺乏</b>的情况下使用。这里，N 是给定数据集的容量。</p>


## 1.6 泛化能力

### 1.6.1 泛化误差

### 1.6.2 泛化误差上界


## 1.7 生成模型与判别模型
<p>监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数或者条件概率分布。</p>
<p>监督学习方法又可以分为生成方法（generative approach）和判别方法（discriminative approach），所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。</p>
<p>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型。<font color="blue">这样的方法之所以称为生成方法，是因为模型表示了给定输入 X 产生输出 Y 的生成关系</font>。典型的生成模型有：<font color="blue">朴素贝叶斯和隐马尔可夫模型</font>。</p>
<p>判别方法由数据直接学习决策函数 f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，即判别模型。判别方法关心的是给定的输入X，应该预测什么样的输出 Y。典型的判别模型包括：k 近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。</p>
<p>生成方法的特点：生成方法可以还原出联合概率分布P(X,Y)，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。</p>
<p>判别方法的特点：判别方法直接学习的是条件概率 P(Y|X) 或决策函数 f(X)，直接面对预测，往往学习的准确率更高；由于直接学习 P(Y|X) 或 f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</p>


## 1.8 分类问题
<p>分类是监督学习的一个核心问题。在监督学习中，当输出变量 Y 取有限个离散值时，预测问题便称为分类问题。这时，<font color="blue">输入变量 X 可以是离散的，也可以是连续的</font>。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier）。分类器对新的输入进行输出的预测（prediction），称为分类（classification）。可能的输出称为类（class）。分类的类别为多个时，称为多类分类问题。</p>
<p>许多统计学习方法可以用于分类，包括k 近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯蒂回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow 等。</p>


## 1.9 标注问题
<p>标注（tagging）也是一个监督学习问题。可以认为标注问题是分类问题的一个推广。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。</p>
<p>标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。</p>


## 1.10 回归问题
<p>回归（regression）是监督学习的另一个重要问题。回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化</p>
<p>回归模型正是表示从输入变量到输出变量之间映射的函数。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。</p>
<p>回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。</p>
<p>回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法（least squares）求解。</p>